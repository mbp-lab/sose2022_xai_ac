{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59239cbb-a912-499c-b0f1-2fb9742217cb",
   "metadata": {},
   "source": [
    "# Explaining Facial Expression Recognition with LIME\n",
    "## Notebook 1:  XAI for Affective Computing (SoSe2022)\n",
    "\n",
    "In this notebook, using the [LIME python package](https://github.com/marcotcr/lime) you will attempt to explain predictions of two Facial Expression recognition models, trained using a sample of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/). AffectNet is a dataset of facial expressions expression in the wild, and is labeled with 8 facial expression categories: **Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, and Contempt**. (If interested, have a look at the paper https://arxiv.org/abs/1708.03985). \n",
    "\n",
    "In **Part 1**, will first explore the explanations of an already trained Convolutional Neural Network trained on raw face images, using LIME Image Explainer.  Then in **Part 2**, we will explore the explanations of a pretrained Random Decision Forest trained on [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/) that were extracted from the face images using [OpenFace2.0](https://github.com/TadasBaltrusaitis/OpenFace). \n",
    "\n",
    "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
    "\n",
    "See **README** To get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2944a-a7fb-4d96-b652-8a2fe4f432d2",
   "metadata": {},
   "source": [
    "## Part 0: Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39040a80-0531-4da1-a98a-bd59f87d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295770a4-0087-4da4-80f6-15d5df6a18a9",
   "metadata": {},
   "source": [
    "##### Import necessary libraries\n",
    "\n",
    "(see README for necessary package installations if you receive a `module not found` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b037e5c-d2f7-4c98-968b-e57231811d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import tensorflow for model loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import sklearn for processing data and results\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# import model loading function\n",
    "from model import cnn_model\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23063e3a-f8ca-4086-b36c-5819efff4868",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c7351-ae5f-4c61-bdc4-fae36aae8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def display_one_image(image, title, subplot, color='black', mask=None):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, )\n",
    "    plt.title(title, fontsize=16)\n",
    "    \n",
    "def display_nine_images(images, titles, preds, start, title_colors=None):\n",
    "    subplot = 331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    for i in range(9):\n",
    "        color = 'black' if title_colors is None else title_colors[i]\n",
    "        idx = start+i\n",
    "        display_one_image(images[idx], f'Actual={titles[idx]} \\n Pred={preds[idx]} \\n Index = {idx}', 331+i, color)\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "def image_title(label, prediction):\n",
    "  # Both prediction (probabilities) and label (one-hot) are arrays with one item per class.\n",
    "    class_idx = np.argmax(label, axis=-1)\n",
    "    prediction_idx = np.argmax(prediction, axis=-1)\n",
    "    if class_idx == prediction_idx:\n",
    "        return f'{CLASS_LABELS[prediction_idx]} [correct]', 'black'\n",
    "    else:\n",
    "        return f'{CLASS_LABELS[prediction_idx]} [incorrect, should be {CLASS_LABELS[class_idx]}]', 'red'\n",
    "\n",
    "def get_titles(images, labels, model):\n",
    "    predictions = model.predict(images)\n",
    "    titles, colors = [], []\n",
    "    for label, prediction in zip(classes, predictions):\n",
    "        title, color = image_title(label, prediction)\n",
    "        titles.append(title)\n",
    "        colors.append(color)\n",
    "    return titles, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3666d-420d-4bf4-b933-fa54646f1d37",
   "metadata": {},
   "source": [
    "## Part 1:  Local Explations of Facial Expression Recognition with Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c5a59-febe-476a-99d3-e7fdbd4e7ff5",
   "metadata": {},
   "source": [
    "##### Set our global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fd431-7494-4db7-a6f9-c070e4be00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 80\n",
    "NUM_CLASSES = 8\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a57a3-5065-4a20-a40b-9d60f30763a5",
   "metadata": {},
   "source": [
    "### Load Pretrained CNN Model and Setup Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2f6c6-c918-405d-80a5-fb4ae74aa07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you've downloaded the models from LernraumPlus (see README instructions for Notebook I)\n",
    "model_path = '../models/affectnet_model_e=60/affectnet_model'\n",
    "\n",
    "# test loading weights\n",
    "model_xai = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model_xai.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73340803-65f0-41e7-a800-de56bdec1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/affectnet/val_class/'\n",
    "# test_dir = '../localdata/affectnet/val_class/'\n",
    "\n",
    "\n",
    "# Load data\n",
    "test_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                  rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical', \n",
    "                                            seed = SEED)\n",
    "images, classes = next(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e29bf-9bbc-4eb3-8fd1-9d9f6c801830",
   "metadata": {},
   "source": [
    "### Evaluation and Predictions\n",
    "Here we evaluate the loaded model to ensure it is working as expected.  You should get around $48.75\\%$ accuracy. While this is not a perfect classifier is well above random guessing which is $1 / 8 * 100 = 12.5$ accuracy\n",
    "\n",
    "Then we load predictions to use throughout the notebook. \n",
    "\n",
    "The predictions results can then be viewed with a confusion matrix to see where the model is confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5d926-6b4a-400b-bf82-4e47c8bdbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model_xai.evaluate(test_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6532b5-e0b6-4d8f-8227-2e4858763eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get softmax predictions from model\n",
    "preds = model_xai(images)\n",
    "\n",
    "# convert predictions to integers\n",
    "y_pred = np.argmax(preds, axis=-1)\n",
    "y_true = np.argmax(classes, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58419a6d-6d1b-4e83-b1b7-6e1625bda5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also review the confusion matrix\n",
    "cm_data = confusion_matrix(y_true, y_pred)\n",
    "cm = pd.DataFrame(cm_data, columns=CLASS_LABELS, index = CLASS_LABELS)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ce90b-8453-4ade-a830-3b7402ab4534",
   "metadata": {},
   "source": [
    "## Task 1: LIME Local Prediction Explanations\n",
    "\n",
    "Now that we have our model setup, we will review the images and predictions to identify a few data instances to explain.  \n",
    "\n",
    "### Task 1.0\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54dad-245d-478d-b2ea-913b9ee193fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 0\n",
    "\n",
    "true_labels = [CLASS_LABELS[idx] for idx in y_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in y_pred]\n",
    "display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e8bf2-6d47-4051-8399-a7e04905e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in this task\n",
    "img_idxs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf361-07c1-457d-bb61-20589a6b25e7",
   "metadata": {},
   "source": [
    "### Task 1.1 Implement a LIME Image Explainer\n",
    "\n",
    "Implement a [LimeImageExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image) instance, you can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb) for help. \n",
    "\n",
    "*Hint*: Use a to loop iterate through your `img_idxs` array to create a seperate explainer instance for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1575-52d8-4929-b2fd-55c8adec047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "from skimage.segmentation import mark_boundaries # used to get boundries from explanation for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fc09d-3549-47ab-b2ec-9fa93bcd6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2bf55-d8bb-4907-8140-208ea94e332a",
   "metadata": {},
   "source": [
    "#### Task 1.1\n",
    "Print the predicted labels for the top $N$ labels as found by explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87a55d-a143-4485-a5d6-389ea7ef5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea942c-41c2-4e5a-912f-963d82240ed3",
   "metadata": {},
   "source": [
    "### Task 1.2: Visualize Explanations\n",
    "Visualize the explanations for each of the 4 data points from LIME using matplotlib's `imshow` function (see above tutorial). (Or pass the explanation to the `display_one_image` function defined above.)\n",
    "\n",
    "*HINT*: Use the `subplot` parameter of the `display_one_image` to plot a 2x2 grid.  The value should be an integer formated as `RCN` where `R` is the number of rows, `C` is the number of columns, and `C` is the number of the image to plot.  For example, `221` means to plot the first image of a 2x2 grid, `222` means the plot the second images, and so forth... (also see `display_nine_images` for example of this usage.)\n",
    "\n",
    "Experimenting with at least 2 different sets of parameters for the explanation visualizations.  For example, view positive and negative contributions, change the number of features for the explation, or try visualizing a heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71a9a8-bd68-4a54-b330-41d611ffd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "# (you can use more than one notebook cell for this task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421706d-4b9a-4ed2-af6b-fec85dd15f84",
   "metadata": {},
   "source": [
    "### Task 1.3 Report your findings?\n",
    "What are your findings? Can you identify any patterns that explain how the model is working? Are you more or less confident in the model's performance after reviewing the explanations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73f8b5-b207-46e6-bdce-e5614ad4871f",
   "metadata": {},
   "source": [
    "Write here findings here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c29d6-23ec-4851-b22f-cf656086f9ed",
   "metadata": {},
   "source": [
    "## PART 2 LIME Submodular Pick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bcdd2-67f3-4926-86bd-97febb99ef9d",
   "metadata": {},
   "source": [
    "Now let's have a look at using LIME's submodular pick (SP-LIME).  Remember the goal of SP-LIME is to identify a set of data instances that maximize the coverage of the explanations.  So the SP-LIME module will identify $N$ instances to explain, then you can plot the explanations.  \n",
    "\n",
    "Unfortunately, SP-LIME does not work with images directly, so now we will work with a dataset of [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/) extracted from the AffectNet dataset using [OpenFace2.0](https://github.com/TadasBaltrusaitis/OpenFace). In this way, we can now apply SP-LIME to our dataset. \n",
    "\n",
    "In this section, we will now use a pretrained [Random Decision Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) as our classifier instead of a Deep Neural Network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fe682-3761-426f-84b1-96c848e294e0",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52557-2b13-4dc9-a4e5-4554c21028c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data from training and evaluation\n",
    "train_csv = '../data/affectnet_aus/train_aus.csv'\n",
    "val_csv = '../data/affectnet_aus/val_aus.csv'\n",
    "\n",
    "# load training and validation data as pandas dataframeas\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_val = pd.read_csv(val_csv)\n",
    "\n",
    "# smaller dataset for explanations (same data as in Task 1)\n",
    "xai_csv = '../data/affectnet_aus/eval_aus.csv'\n",
    "df_xai = pd.read_csv(xai_csv)\n",
    "\n",
    "# get only the columns storing action units from the dataframe\n",
    "feature_cols = [col for col in df_val if col.startswith('AU')]\n",
    "\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']  # same class labels as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376488e0-2f8b-4217-a004-df945ca39d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data from dataframe to Numpy arrays\n",
    "\n",
    "X_train = np.array(df_train.loc[:, feature_cols])\n",
    "y_train = np.array(df_train['class'])\n",
    "\n",
    "X_test = np.array(df_val.loc[:, feature_cols])\n",
    "y_test = np.array(df_val['class'])\n",
    "\n",
    "X_xai = np.array(df_xai.loc[:, feature_cols])\n",
    "y_xai = np.array(df_xai['class'])\n",
    "\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)\n",
    "print('XAI', X_xai.shape, y_xai.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c3337-15c4-487e-8ed7-b2923e288481",
   "metadata": {},
   "source": [
    "### Load pretrained RDF model\n",
    "And validate that it works.  \n",
    "The accuracy of the model should be around $99.65%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296039e-4102-4655-8250-c8a295effebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/affect_rdf.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabfd65-9a24-46c9-b274-b3cf73579bce",
   "metadata": {},
   "source": [
    "#### Now evaluate on full test data\n",
    "Unfortunately, the accuracy is only $43\\%$ but this is well above chance guessing which would get $1 / 8 * 100 = 12.5%$ accuracy (since there are 8 total classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d15e6-7c89-45c1-b5cf-f1af4a3b3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_test_preds = clf.predict(X_test)\n",
    "y_test_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ba403-9695-4352-b161-53727c7cfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_true, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62c5d7-d40f-47da-9569-9c1b22b88ffa",
   "metadata": {},
   "source": [
    "We can also review the confusion matrix to see where the model is confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da3b65-0589-448b-924b-6d403da80be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_test_true, y_test_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=CLASS_LABELS, index=CLASS_LABELS)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660770b5-73d7-47a8-8772-bc4025f608d8",
   "metadata": {},
   "source": [
    "#### Evaluate in XAI Data\n",
    "The XAI data is a subset of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176335d5-5ebf-42b5-9880-6450e490b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "y_xai_preds = clf.predict(X_xai)\n",
    "y_xai_true = y_xai\n",
    "print(classification_report(y_xai_true, y_xai_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc742dd3-95ce-499e-8046-607bd945b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data = confusion_matrix(y_xai_true, y_xai_preds)\n",
    "cm = pd.DataFrame(cm_data, columns=CLASS_LABELS, index=CLASS_LABELS)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4f0d0-191a-43ad-ae43-7fba70ce455a",
   "metadata": {},
   "source": [
    "### TASK 2: SP-LIME Implementation  \n",
    "\n",
    "Now on to the implementation of SP-LIME. \n",
    "\n",
    "#### Task 2.0: Identify Some Images to Explain\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "### Task 2.0\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction\n",
    "    - can be the same, or different, as in task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9695273-8d13-46d7-85f3-6391cbcc8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 10\n",
    "\n",
    "images = [io.imread(f) for f in df_xai.image]\n",
    "\n",
    "true_labels = [CLASS_LABELS[idx] for idx in y_xai_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in y_xai_preds]\n",
    "display_nine_images(images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c582e-8524-4dce-b912-63582c2927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in the task\n",
    "img_idxs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcba07-5caa-43f0-96af-acc5fbe43bc0",
   "metadata": {},
   "source": [
    "#### TASK 2.1\n",
    "Implement a [LimeTabularExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular), you can review the [LIME tutorial](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) for help.\n",
    "\n",
    "*Hint*: Use a to loop iterate through your `img_idxs` array to create a seperate explainer instance for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed07cf-1bc4-4d52-a9d0-3735a3407991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68d4d3-f0b7-4688-bc52-ce6b99b9af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e392e82-ffdb-4a2d-ac4a-eb39694837d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TASK 2.2\n",
    "Review at the previously identified 4 data instances from the `X_xai` dataset, by getting an explanation from the tabular explainer, and then plotting the explanations for each data instance (see tutorial mentioned above).  \n",
    "\n",
    "HINT: Use the subplot parameter of the display_one_image to plot a 2x2 grid. The value should be an integer formated as RCN where R is the number of rows, C is the number of columns, and C is the number of the image to plot. For example, 221 means to plot the first image of a 2x2 grid, 222 means the plot the second images, and so forth... (also see display_nine_images for example of this usage.)\n",
    "\n",
    "Make sure to print out the **True** and **Predicted** labels for each instance.\n",
    "\n",
    "Try experimenting with different parameters for the explainer and explanation.\n",
    "\n",
    "**Bonus Task: Include images**   \n",
    "The data frame includes paths to the images (in the column named `image`) that correspond with the AU features.  Load the images so you can compare the AUs with the actual data. (you can use [`skimage io`](https://scikit-image.org/docs/dev/user_guide/getting_started.html) to load the image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654e83e-86d5-45fd-a393-45cd7d41664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47833c07-aa32-4dec-99d7-966b9ef96c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(df_val.iloc[i]['image'].replace('data', 'localdata'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec2738-bedc-4ed5-8092-a652b398ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92c118-06c4-420d-988c-499d10f4b941",
   "metadata": {},
   "source": [
    "#### TASK 2.3\n",
    "Identify the important Facial Actions Units and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these local explanations provide? How does this compare with the image explanations from Task 1?\n",
    "\n",
    "Note: In the feature names, you will see features with a `_c` and a `_r` at the end.  The `_r` means the intentsity of the action unit (i.e., how strong is it's presence), and the `_c` is a binary feature indicating the presence (value=1), or non-presence (value=0), of an action unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f879e0-7cf0-4cf8-b963-52fc3b376fe6",
   "metadata": {},
   "source": [
    "Write your answer here...\n",
    "\n",
    "25 = lip opener\n",
    "12 = lip corner puller\n",
    "04 = brow lowerer\n",
    "14 = dimpler\n",
    "02 = outer brow raiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28bca1-75f1-440d-8ca0-cfdc45efbc68",
   "metadata": {},
   "source": [
    "#### Task 2.4\n",
    "\n",
    "Now implement [Submodular Pick](https://lime-ml.readthedocs.io/en/latest/lime.html#lime-submodular-pick-module) instance to get try to gain a global perspective of how the model makes decisions. You can review the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Submodular%20Pick%20examples.ipynb)\n",
    "\n",
    "Try setting `num_exps_desired` to 16 to try to get 2 examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1190bc2-9f10-46ae-a0ad-2f86ad1eef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e607471-9d58-4d10-9334-7443ad72c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978aca38-c0d0-4f02-af8f-44d4a5e39c41",
   "metadata": {},
   "source": [
    "#### Task 2.5\n",
    "Now plot the explantions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9e27c-15de-438f-a9e9-afc135d2919d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc61acd-641e-4691-ab87-62ef7f8680f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TASK 2.6\n",
    "\n",
    "Again identify important AUs for the classes and compare with the images at [Facial Action Units](https://imotions.com/blog/facial-action-coding-system/).  What insights do these explanations provide? Do you know have a better understanding of how the model is working? If not, what is lacking using the LIME approach and/or what could be done differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88389d57-9fac-423b-bdc7-50eede30dae4",
   "metadata": {},
   "source": [
    "### Task 3: Final Discussion\n",
    "Between tasks 1 and 2, which of the 2 models and methods best support explainablity as we've discussed throughout the seminar. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730cce4-3b0a-4709-836e-15fe01acebe5",
   "metadata": {},
   "source": [
    "write your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
