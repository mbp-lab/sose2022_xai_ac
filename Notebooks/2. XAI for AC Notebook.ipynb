{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59239cbb-a912-499c-b0f1-2fb9742217cb",
   "metadata": {},
   "source": [
    "# Explaining Facial Expression Recognition\n",
    "## Notebook 2:  XAI for Affective Computing (SoSe2022)\n",
    "\n",
    "Description here\n",
    "\n",
    "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
    "\n",
    "Make sure to read the **README** beforing starting this notebook, to review the required Python packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2944a-a7fb-4d96-b652-8a2fe4f432d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 0: Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39040a80-0531-4da1-a98a-bd59f87d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295770a4-0087-4da4-80f6-15d5df6a18a9",
   "metadata": {},
   "source": [
    "##### Import necessary libraries\n",
    "\n",
    "(see README for necessary package installations if you receive a `module not found` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b037e5c-d2f7-4c98-968b-e57231811d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import tensorflow for model loading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# required for ALIBI Contrastive Explanation Methods (CEM)\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "\n",
    "# import sklearn for processing data and results\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# import model loading function\n",
    "from model import cnn_model\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23063e3a-f8ca-4086-b36c-5819efff4868",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Helper functions for plotting faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c7351-ae5f-4c61-bdc4-fae36aae8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def display_one_image(image, title, subplot, color='black', mask=None):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, )\n",
    "    plt.title(title, fontsize=16)\n",
    "    \n",
    "def display_nine_images(images, titles, preds, start, title_colors=None):\n",
    "    subplot = 331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    for i in range(9):\n",
    "        color = 'black' if title_colors is None else title_colors[i]\n",
    "        idx = start+i\n",
    "        display_one_image(images[idx], f'Actual={titles[idx]} \\n Pred={preds[idx]} \\n Index = {idx}', 331+i, color)\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "def image_title(label, prediction):\n",
    "  # Both prediction (probabilities) and label (one-hot) are arrays with one item per class.\n",
    "    class_idx = np.argmax(label, axis=-1)\n",
    "    prediction_idx = np.argmax(prediction, axis=-1)\n",
    "    if class_idx == prediction_idx:\n",
    "        return f'{CLASS_LABELS[prediction_idx]} [correct]', 'black'\n",
    "    else:\n",
    "        return f'{CLASS_LABELS[prediction_idx]} [incorrect, should be {CLASS_LABELS[class_idx]}]', 'red'\n",
    "\n",
    "def get_titles(images, labels, model):\n",
    "    predictions = model.predict(images)\n",
    "    titles, colors = [], []\n",
    "    for label, prediction in zip(classes, predictions):\n",
    "        title, color = image_title(label, prediction)\n",
    "        titles.append(title)\n",
    "        colors.append(color)\n",
    "    return titles, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa542afb-62f4-4e6e-a2eb-b0c5228f7a83",
   "metadata": {},
   "source": [
    "##### Some Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c3576-d1b6-4895-bb42-cd665ce6c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 80 # set to 80 to easily load all images using image generator in one call\n",
    "NUM_CLASSES = 8\n",
    "CLASS_LABELS = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15372d-c8d6-4aed-976a-06248e900a7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213a43f-4450-4e62-82c3-0dbaebb77b04",
   "metadata": {},
   "source": [
    "### CNN Model - Facial Expression Recognition from Images\n",
    "This model is a pretrained Convolutional Neural Network for Facial Expression Recognition (FER) trained on raw images of people making facial expressions, from a subset of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/). \n",
    "\n",
    "This is the same model as our previous notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290b787-c964-436f-b485-dcf37b4b09a3",
   "metadata": {},
   "source": [
    "#### Load the pretrained model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf02890-29f8-4b49-a244-a0b20b1e0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you've downloaded the models from LernraumPlus (see README instructions for Notebook I)\n",
    "model_path = '../models/affectnet_model_e=60/affectnet_model'\n",
    "\n",
    "# test loading weights\n",
    "fer_cnn_model = cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "fer_cnn_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3268f-e3dd-437d-a8da-ca17e00e119f",
   "metadata": {},
   "source": [
    "#### Load the images to be explained\n",
    "`ImageDataGenerator` is a [Keras utility class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to easily load images for processing with a Keras model.\n",
    "\n",
    "The numpy array `X_img` contains 80 images that we will use for explanations.  And `Y_img_true` stores the ground truth labels, encoded as [one hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26318c03-1852-4d46-8497-043669dda7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/affectnet/val_class/'\n",
    "# test_dir = '../localdata/affectnet/val_class/'\n",
    "\n",
    "\n",
    "# Load data\n",
    "test_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                  rescale=1./255)\n",
    "test_gen = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical', \n",
    "                                            seed = SEED)\n",
    "X_img, Y_img_true = next(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec44a20-53df-47e5-ad64-6c0c6d334579",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "Here we evaluate the loaded model to ensure it is working as expected.  You should get around $48.75\\%$ accuracy. While this is not a perfect classifier is well above random guessing which is $1 / 8 * 100 = 12.5$ accuracy\n",
    "\n",
    "This is the same CNN model as before, so refer to our previous notebook to view more details on its performance.\n",
    "\n",
    "We also get predictions of the model for the dataset, stored in `Y_img_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c6f3a-b682-4f8d-9c6d-6dcf83e29712",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = fer_cnn_model.evaluate(test_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9562bbe-5f4b-4f54-81c9-e940130f8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_img_pred = fer_cnn_model.predict(X_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bc59c-0383-40f4-b9a7-6dd9be9f36cc",
   "metadata": {},
   "source": [
    "### Random Decision Forest Model - Facial Expression Recognition from Facial Action Units\n",
    "\n",
    "This model is a pretrained random decision forest (RDF) trained on the facial action units (FAUs) of people expression emotions extracted from the AffectNet dataset using [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace).\n",
    "\n",
    "Again, this is the same model as our previous Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4f13e-d5ff-4796-9f07-bfef609008f4",
   "metadata": {},
   "source": [
    "#### Load the model\n",
    "\n",
    "This model is a Random Decision Forest (implemented with Scikit-Learn) trained on FAUs extracted from the AffectNet dataset using OpenFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e544a7-f251-486a-a9f4-a366a08b9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/affect_rdf.pkl', 'rb') as f:\n",
    "    fer_rdf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76485416-6e43-469c-ae60-edbddee085b2",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "Here we load the preextracted FAUs from a `csv` created by OpenFace during FAU extraction of the images from the small subset of the AffectNet dataset.  We load the data into a Pandas Dataframe, then convert the columns Numpy array arrays for easier processing with scikit-learn.\n",
    "\n",
    "The numpy array `X_aus` contains FAUs from the 80 images available for explainations.  And `Y_aus_true` stores the ground truth labels, encoded as [one hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), for each set of FAUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56b3d8-28e9-457c-9eef-275b793b288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_csv = '../data/affectnet_aus/train_aus.csv'\n",
    "df_aus_train = pd.read_csv(train_csv)\n",
    "\n",
    "# Small dataset for explanations\n",
    "xai_csv = '../data/affectnet_aus/eval_aus.csv'\n",
    "df_aus_xai = pd.read_csv(xai_csv)\n",
    "\n",
    "# get only the columns storing action units from the dataframe\n",
    "feature_cols = [col for col in df_aus_xai if col.startswith('AU')]\n",
    "\n",
    "X_aus = np.array(df_aus_xai.loc[:, feature_cols])\n",
    "Y_aus_true = np.array(df_aus_xai['class'])\n",
    "\n",
    "print('XAI Dataset', X_aus.shape, Y_aus_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e4945-b874-4048-b999-60b91291e7de",
   "metadata": {},
   "source": [
    "#### Evaluate the Dataset\n",
    "\n",
    "Here we evaluate the performance of the RDF Classifier on on the `X_aus` dataset. The accuracy should be around $42\\%$\n",
    "\n",
    "- This is the same dataset for the last notebook. If you want to review more results (such as full test data or confusion matrices), please review your previous notebook.\n",
    "\n",
    "We also get predictions of the model for the dataset, stored in `Y_aus_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0435a4-2a2d-4e32-9302-0099414ae121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions\n",
    "print(f'{fer_rdf_model.score(X_aus, Y_aus_true) * 100:0.2f}% Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9038369-38d3-41fa-ab4d-090e069c4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_aus_pred = fer_rdf_model.predict(X_aus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ef2b9-783f-4ec6-9fe6-bd435be11a7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART 2: Review the Datasets\n",
    "\n",
    "Now that we have our model setup, we will review the images and predictions to identify a few data instances to explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80927e-a13b-4e93-bcfb-2f0f0ced1eb0",
   "metadata": {},
   "source": [
    "### Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd54dad-245d-478d-b2ea-913b9ee193fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 40\n",
    "\n",
    "# gets labels for ground truth and predictions\n",
    "true_labels = [CLASS_LABELS[idx] for idx in np.argmax(Y_img_true, axis=1)]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in np.argmax(Y_img_pred, axis=1)]\n",
    "\n",
    "display_nine_images(X_img, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb0b60-7732-46ff-969b-0d9a6693c14d",
   "metadata": {},
   "source": [
    "### FAU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69be0d-182a-41c0-b58b-3335bb71caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays first 9 images in array\n",
    "start = 40\n",
    "\n",
    "# Gets all images from folder\n",
    "fau_images = [io.imread(f) for f in df_aus_xai.image]\n",
    "\n",
    "# gets labels for ground truth and predictions\n",
    "true_labels = [CLASS_LABELS[idx] for idx in Y_aus_true]\n",
    "pred_labels = [CLASS_LABELS[idx] for idx in Y_aus_pred]\n",
    "\n",
    "display_nine_images(fau_images, true_labels, pred_labels, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c29d6-23ec-4851-b22f-cf656086f9ed",
   "metadata": {},
   "source": [
    "## PART 3: Explanations with DiCE\n",
    "In this part of the notebook, you will generate Counterfactual Explanations using the Python Library, [Diverse Counterfactual Explanations (DiCE)](http://interpret.ml/DiCE/).\n",
    "\n",
    "Counterfactual explanations typically work on tabular data, so in this part we will be using the FAU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbd0f8-43bb-4adc-b5dd-e45e11a35051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43342dbc-b953-4ccb-bfc2-8bfb21397401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify categorical and numerical features as needed by DiCE\n",
    "categorical_features = list(df_aus_train.columns[df_aus_train.columns.str.contains('_c')])\n",
    "numerical_features = list(df_aus_train.columns[df_aus_train.columns.str.contains('_r')])\n",
    "\n",
    "all_features = numerical_features + categorical_features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc272d8-5af1-48e9-9ebb-c0364029aa30",
   "metadata": {},
   "source": [
    "#### Task 3.0: Identify Some Images to Explain\n",
    "- Review the FAU dataset using the helper code in part 2 above.\n",
    "- Try changing start value to get a new set of images (there are 10 images for each class, so for example, the class happy will be at indexes 10-19)\n",
    "- Search through the images to find at least 4 to explain \n",
    "    - Find classes that you would like to explain, and from each class select 2 images\n",
    "        - one should be a correct prediction  \n",
    "        - and one should be an incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c582e-8524-4dce-b912-63582c2927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Enter the Indexes Here ### \n",
    "###############################\n",
    "# you will use this array later in the task\n",
    "img_idxs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcba07-5caa-43f0-96af-acc5fbe43bc0",
   "metadata": {},
   "source": [
    "### Task 3.1 \n",
    "In this task, you will use DiCE to generate a set counterfactual explanations for your selected instances.\n",
    "\n",
    "#### Task 3.1.1 Setup a DiCE explainer instance\n",
    "\n",
    "See the [intro to DiCE](http://interpret.ml/DiCE/notebooks/DiCE_getting_started.html) for details on working with this library.\n",
    "\n",
    "Note: data requires requires dataframe for creating explainers and explanations. \n",
    "- for setting up the explainer you can use the following to create a dataframe of features and classes from the training data\n",
    "    - `df_aus_train[all_features+['class']`\n",
    "- for generating instances to explain, you can use the following code:\n",
    "    - `df_aus_xai[all_features][40:41]` where 40 is the index of instance to explan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1880d62-b8fb-45e5-a931-6156f4424e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9742cc-6ed3-4649-9cf2-5388d3aa95f1",
   "metadata": {},
   "source": [
    "#### Task 3.1.2: Use the Explainer to Generate Counterfactual Explanations\n",
    "\n",
    "Generate counterfacutal explanations for each of your select data instances from task 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8be0c9-1800-45c8-af7c-b1d11c12fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4b40b-8340-45fd-8724-e0219bb5c758",
   "metadata": {},
   "source": [
    "#### Task 3.1.3: Visualize Counterfactuals\n",
    "\n",
    "Now visualize the images as Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94991f83-2a6b-4a76-a687-4a745f9b4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "pd.set_option('display.max_columns', None) # so that Jupyter doesn't truncate columns of dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1b3c4-4eb7-42ab-83f1-4a7ce766d447",
   "metadata": {},
   "source": [
    "#### Task 3.1.4: Describe your observations\n",
    "\n",
    "![Action Units](./assets/fac.jpg)\n",
    "\n",
    "1. Which features are most important for the detection of the specific facials expressions of your data instances?  Do the counterfactuals make sense according to your intuition of the contrastive expression class you're using\n",
    "2. The `generate_counterfactuals` method has a parameter `features_to_vary` so that we can restrict which features are perturbed in CF generation.  Are there any AUs that shouldn't be perturbed for our task of emotion detection? Why or why not? Additionally, should we set `permitted_range` parameter to limit the ranges of our continous features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436b908-71fe-40b5-9b7c-9d2510c26bd8",
   "metadata": {},
   "source": [
    "Type your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d636b-c216-4f63-8cb1-5043beae5673",
   "metadata": {},
   "source": [
    "### Task 3.2 Use the DiCE Explainer to Generate Feature Attribution Scores\n",
    "\n",
    "DiCE can also generate [local and global feature attribution scores](http://interpret.ml/DiCE/notebooks/DiCE_getting_started.html#Generating-feature-attributions-(local-and-global)-using-DiCE) based on the identified counterfactuals.  In this task, we will do just that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7244e11-93c0-4b58-8d88-4bfbe3e045e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting importance dictionaries provided by DiCE\n",
    "def plot_importance_dict(importance_dict):\n",
    "    keys = list(importance_dict.keys())\n",
    "    vals = [float(importance_dict[k]) for k in keys]\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.barplot(x=keys, y=vals)\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715ce97-48b3-4bd7-9376-7314cd81942e",
   "metadata": {},
   "source": [
    "#### Task 3.2.1 Generate and Plot Local Importance Scores\n",
    "\n",
    "Using your previously defined DiCE explainer, generate and plot local importance scores your your data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa78c06-0505-4d8f-a29c-0c71763a8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6eec07-dbe6-4771-9027-c919c5be1227",
   "metadata": {},
   "source": [
    "#### Task 3.2.2 Generate and Plot Local Importance Scores\n",
    "\n",
    "Using your previously defined DiCE explainer, generate and plot global importance scores your your data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf6994-e130-4eb0-8b26-80b4fa5e2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f23e16-63ba-430f-99f8-d29790c1d9f6",
   "metadata": {},
   "source": [
    "#### Task 3.2.3 Describe your findings\n",
    "\n",
    "![Action Units](./assets/fac.jpg)\n",
    "\n",
    "1. How does DiCE calculate feature importance from counterfactuals?\n",
    "1. Do the plots lead to any interesting insights regarding AUs or facial expression detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c198c-f72a-4d8c-942b-b85347f2e69f",
   "metadata": {},
   "source": [
    "Write your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71e6bf-14ed-4b36-8c78-15293f684306",
   "metadata": {},
   "source": [
    "## Part 4 Explanations with the Alibi Inspection and Interpretation Library\n",
    "\n",
    "[Alibi Explain](https://docs.seldon.io/projects/alibi/en/stable/index.html) is an open source python package for interpreting and inspecting machine learning models.  It contains many implementations of many highly relevant XAI methods, including ones that we have discussed through the sememster.  \n",
    "\n",
    "In this part, you will select with explanation methods you would like to implement. To this end, you will review the documentation and [avaialble algorithms](https://docs.seldon.io/projects/alibi/en/stable/overview/algorithms.html) of Alibi and choose 2 methods to implement.  After implementation you will describe your findings and answer a few questions regarding the method. You can choose to use the image dataset with the CNN model, the FAU dataset with the RDF model, or both for your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea55fda-03d7-450e-a309-14819ea4cbc0",
   "metadata": {},
   "source": [
    "#### Task 4.1.1: Implement 1 Explanation Method from Alibi\n",
    "\n",
    "Select one XAI method from the list of avaialable XAI algorithms supported by Alibi.  Read the documention from Alibi regarding that method. The implment and explain at least one prediction using that method.  \n",
    "\n",
    "In some cases, you will have some hyperparameters to select.  Try to evalaution a few different values for the hyperparameter to see their effects.  \n",
    "\n",
    "**(Optional)** To assist with this, I have provided a function, `setup_gridsearch` for creating a grid of hyperparameters making a grid search easier.  To use this function simply pass it any number of python lists contain the values for HPs.  The function will create a list of tuples containing all possible combinations of HPs that you can then iterate over. Using grid search may take awhile depending on the time it takes to create one explanation.  \n",
    "\n",
    "Example:\n",
    "\n",
    "``` python\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "kappas = [0.01, 0.1, 1, 10]\n",
    "betas = [0.001, 0.1, .9]\n",
    "\n",
    "grid = setup_gridsearch(lrs, kappas, betas)\n",
    "\n",
    "exps = []\n",
    "for lr, kappa, beta in (pbar := tqdm.tqdm(grid)):\n",
    "    pbar.set_description(f'{lr=}, {kappa=}, {beta}')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd747a6c-bcdd-466c-813d-9851ec5d6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def setup_gridsearch(*args):\n",
    "    return list(itertools.product(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc0a36-8c29-4abf-851f-6f41d678efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6841bb6-d0aa-4953-afcb-89259c29d70b",
   "metadata": {},
   "source": [
    "#### Task 4.1.2: Describe your implementation and findings\n",
    "\n",
    "Answer the following questions regarding your implementation\n",
    "\n",
    "1. Why did you select this method?\n",
    "2. What hyperparameters are required? And what do they mean for the algorithm?  \n",
    "3. Did you notice any effects of changing the HPs?\n",
    "4. What does the output of this method represent?\n",
    "5. We your able to identify any interesting findings regarding the dataset or explained prediction(s)?\n",
    "6. Any other points regarding this method you found interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f102ba6-fec3-4293-902a-1509b8b21fdd",
   "metadata": {},
   "source": [
    "Write your answers here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca5dfd-1769-4497-a914-7aea26abeef7",
   "metadata": {},
   "source": [
    "#### Task 4.2.1: Implement a Different Explanation Method from Alibi\n",
    "\n",
    "Select one XAI method from the list of avaialable XAI algorithms supported by Alibi.  Read the documention from Alibi regarding that method. The implment and explain at least one prediction using that method.  \n",
    "\n",
    "In some cases, you will have some hyperparameters to select.  Try to evalaution a few different values for the hyperparameter to see their effects.  (see previous task for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef8f53-4b69-44b5-b090-f0eff065d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE GOES HERE #####\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19eae0a-ff2b-4a88-bbc4-b71494313657",
   "metadata": {},
   "source": [
    "#### Task 4.2.2: Describe your implementation and findings\n",
    "\n",
    "Answer the following questions regarding your implementation\n",
    "\n",
    "1. Why did you select this method?\n",
    "2. What hyperparameters are required? And what do they mean for the algorithm?  \n",
    "3. Did you notice any effects of changing the HPs?\n",
    "4. What does the output of this method represent?\n",
    "5. We your able to identify any interesting findings regarding the dataset or explained prediction(s)?\n",
    "6. Any other points regarding this method you found interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c4d8f-4f2c-43c3-acf6-2386b17749d9",
   "metadata": {},
   "source": [
    "Write your answers here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
